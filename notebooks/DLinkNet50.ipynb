{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7ac14a50"},"outputs":[],"source":["# Use Google Colab\n","use_google_colab = True\n","# Process the training dataset\n","training_data_processing = True\n","# Train the model\n","model_training = True\n","# Validation the model\n","model_validation = False\n","# Load the model from your Google Drive or local file system\n","model_loading = False"],"id":"7ac14a50"},{"cell_type":"code","execution_count":null,"metadata":{"id":"05dfeb22"},"outputs":[],"source":["if use_google_colab:\n","    from google.colab import drive\n","    from google.colab import files\n","    # Mount your Google Drive on your runtime\n","    drive.mount('/content/gdrive')"],"id":"05dfeb22"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e03cc21e"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import torch\n","from torch import nn\n","from torchvision import models\n","from torch.utils.data import DataLoader, TensorDataset\n","from PIL import Image\n","from skimage.transform import resize\n","import scipy\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"],"id":"e03cc21e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5JSCfmd7YW_"},"outputs":[],"source":["if use_google_colab:\n","    path_training = '/content/gdrive/MyDrive/proj2_data/training/'\n","    path_testing = '/content/gdrive/MyDrive/proj2_data/test_set_images/'\n","    path_data = '/content/gdrive/MyDrive/proj2_data/data/'\n","    path_model = '/content/gdrive/MyDrive/proj2_data/models/'\n","else:\n","    path_training = 'training/'\n","    path_testing = 'test_set_images/'\n","    path_data = 'data/'\n","    path_model = 'models/'"],"id":"z5JSCfmd7YW_"},{"cell_type":"markdown","metadata":{"id":"b12b480d"},"source":["# Get Device for Training"],"id":"b12b480d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"db1c96b5"},"outputs":[],"source":["# Determine if your system supports CUDA\n","cuda_available = torch.cuda.is_available()\n","if cuda_available:\n","    print('CUDA is available. Utilize GPUs for computation')\n","    device = torch.device(\"cuda\")\n","else:\n","    print('CUDA is not available. Utilize CPUs for computation.')\n","    device = torch.device(\"cpu\")"],"id":"db1c96b5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"37344ed4"},"outputs":[],"source":["# Get the GPU information\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","else:\n","    print(gpu_info)"],"id":"37344ed4"},{"cell_type":"markdown","metadata":{"id":"dcd0e4b0"},"source":["# Define the Neural Network"],"id":"dcd0e4b0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"00bec1a0"},"outputs":[],"source":["\"\"\"\n","DLinkNet50.py - Define the neural network for D-LinkNet50.\n","Reference - https://openaccess.thecvf.com/content_cvpr_2018_workshops/w4/html/Zhou_D-LinkNet_LinkNet_With_CVPR_2018_paper.html\n","\"\"\"\n","\n","class dilation_block(nn.Module):\n","    # Instantiate all the modules\n","    def __init__(self,channel):\n","        super(dilation_block, self).__init__()\n","        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)\n","        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=2, padding=2)\n","        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=4, padding=4)\n","        self.dilate4 = nn.Conv2d(channel, channel, kernel_size=3, dilation=8, padding=8)\n","        self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","        \n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.relu3 = nn.ReLU(inplace=True)\n","        self.relu4 = nn.ReLU(inplace=True)\n","        self.relu5 = nn.ReLU(inplace=True)\n","\n","    # Define the block structure                \n","    def forward(self, x):\n","        \"\"\"\n","        dilation_block's forward function.\n","        Args:\n","            x (tensor): input tensor\n","        Returns:\n","            o1 (tensor): the output of this block after processing\n","        \"\"\"\n","        d1 = self.relu1(self.dilate1(x))\n","        d2 = self.relu2(self.dilate2(d1))\n","        d3 = self.relu3(self.dilate3(d2))\n","        d4 = self.relu4(self.dilate4(d3))\n","        d5 = self.relu5(self.dilate5(d4))\n","        \n","        o1 = x + d1 + d2 + d3 + d4 + d5\n","        return o1\n","\n","\n","class decoder_block(nn.Module):\n","    # Instantiate all the modules\n","    def __init__(self, in_channels, out_channels):\n","        super(decoder_block,self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels // 4, kernel_size=1),\n","            nn.BatchNorm2d(in_channels // 4),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=3, \n","                               stride=2, padding=1, output_padding=1),\n","            nn.BatchNorm2d(in_channels // 4),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.Conv2d(in_channels // 4, out_channels, kernel_size=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    # Define the block structure\n","    def forward(self, x):\n","        \"\"\"\n","        decoder_block's forward function.\n","        Args:\n","            x (tensor): input tensor\n","        Returns:\n","            x (tensor): the output of this block after processing\n","        \"\"\"\n","        x = self.block(x)\n","        return x\n","\n","\n","class DLinkNet50(nn.Module):\n","    # Instantiate all the modules\n","    def __init__(self):\n","        super(DLinkNet50, self).__init__()\n","        # Construct a ResNet-50 architecture from https://arxiv.org/pdf/1512.03385.pdf\n","        # Return a model pre-trained on ImageNet\n","        resnet = models.resnet50(pretrained=True)\n","\n","        # Input Block\n","        self.input_block = nn.Sequential(*list(resnet.children())[0:4])\n","\n","        # Encoder Blocks\n","        self.encoder1 = nn.Sequential(*list(resnet.children())[4])\n","        self.encoder2 = nn.Sequential(*list(resnet.children())[5])\n","        self.encoder3 = nn.Sequential(*list(resnet.children())[6])\n","        self.encoder4 = nn.Sequential(*list(resnet.children())[7])\n","        \n","        # Dilation Block\n","        self.dilation = dilation_block(2048)\n","\n","        # Decoder Blocks\n","        self.decoder4 = decoder_block(2048, 1024)\n","        self.decoder3 = decoder_block(1024, 512)\n","        self.decoder2 = decoder_block(512, 256)\n","        self.decoder1 = decoder_block(256, 256)\n","\n","        # Output Block\n","        self.output_block = nn.Sequential(\n","            nn.ConvTranspose2d(256, 32, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    # Define the network structure\n","    def forward(self, x):\n","        \"\"\"\n","        DLinkNet50's forward function.\n","        Args:\n","            x (tensor): input tensor\n","        Returns:\n","            o1 (tensor): the output of this model after processing\n","        \"\"\"\n","        # Input\n","        i1 = self.input_block(x)\n","\n","        # Encoding\n","        e1 = self.encoder1(i1)\n","        e2 = self.encoder2(e1)\n","        e3 = self.encoder3(e2)\n","        e4 = self.encoder4(e3)\n","        \n","        # Dilation\n","        e4 = self.dilation(e4)\n","\n","        # Decoding\n","        d4 = self.decoder4(e4) + e3\n","        d3 = self.decoder3(d4) + e2\n","        d2 = self.decoder2(d3) + e1\n","        d1 = self.decoder1(d2)\n","\n","        # Output\n","        o1 = self.output_block(d1)\n","\n","        return o1 "],"id":"00bec1a0"},{"cell_type":"markdown","metadata":{"id":"68b4d932"},"source":["# Create an Instance of the Neural Network"],"id":"68b4d932"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac408fb2"},"outputs":[],"source":["model = DLinkNet50()\n","if cuda_available:\n","    # Move the model to GPU\n","    model.cuda()\n","print(model)"],"id":"ac408fb2"},{"cell_type":"markdown","metadata":{"id":"51deced5"},"source":["# Load and Process the Training Dataset"],"id":"51deced5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"75a606eb"},"outputs":[],"source":["# The resolution of resized training images and the corresponding masks\n","training_resize = 384\n","# The number of resized training pairs used for data augmentation\n","training_number = 100\n","# The resolution of resized testing images\n","testing_resize = int(608 * training_resize / 400)\n","if testing_resize % 2 == 1:\n","    testing_resize += 1"],"id":"75a606eb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ee7b917"},"outputs":[],"source":["def training_data_loading(path_training='training/', training_resize=384, training_number=100):\n","    \"\"\"\n","    training_data_loading - Load and generate the resized training dataset and validation dataset.\n","    Args:\n","        path_training (str): the location in your Google Drive or local file system\n","        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs) (default: 384)\n","        training_number (int): the number of resized training pairs used for data augmentation (default: 100)\n","    Returns:\n","        images_training, labels_training (numpy): the resized training dataset\n","        images_validation, labels_validation (numpy): the resized validation dataset\n","    \"\"\"\n","    images_loading = np.empty(shape=(100, 3, training_resize, training_resize))\n","    labels_loading = np.empty(shape=(100, 1, training_resize, training_resize))\n","    \n","    for index in tqdm(range(1, 101)):\n","        # Load a training pair\n","        image = np.array(Image.open(f'{path_training}images/satImage_{str(index).zfill(3)}.png')).astype(float) / 255\n","        label = np.array(Image.open(f'{path_training}groundtruth/satImage_{str(index).zfill(3)}.png')).astype(float) / 255\n","        \n","        # Expand the shape of the mask\n","        label = np.expand_dims(label, 2)\n","        \n","        # Resize the training pair\n","        image = resize(image, (training_resize, training_resize))\n","        label = resize(label, (training_resize, training_resize))\n","        \n","        # Reverse the axes of the resized training pair\n","        image = np.transpose(image, (2, 0, 1))\n","        label = np.transpose(label, (2, 0, 1))\n","        \n","        images_loading[index-1] = image\n","        labels_loading[index-1] = label\n","    \n","    # Permute the resized training dataset randomly\n","    permuted_sequence = np.random.permutation(100)\n","    images_loading = images_loading[permuted_sequence]\n","    labels_loading = labels_loading[permuted_sequence]\n","    \n","    # Generate the resized training dataset and validation dataset\n","    images_training = images_loading[:training_number]\n","    labels_training = labels_loading[:training_number]\n","    images_validation = images_loading[training_number:]\n","    labels_validation = labels_loading[training_number:]\n","    \n","    return images_training, labels_training, images_validation, labels_validation\n","\n","\n","def training_data_augmentation(images_training, labels_training, rotations, flips, shifts, training_resize=384):\n","    \"\"\"\n","    training_data_augmentation - Generate the augmented training dataset.\n","    Args:\n","        images_training, labels_training (numpy): the resized training dataset\n","        rotations (list): the parameters for rotating resized training images and their corresponding masks (training pairs)\n","        flips (list): the parameters for flipping rotated training pairs\n","        shifts (list): the parameters for shifting flipped training pairs\n","        training_resize (int): the resolution of resized training pairs (default: 384)\n","    Returns:\n","        images_augmented, labels_augmented (numpy): the augmented training dataset\n","    \"\"\"\n","    num_rota = len(rotations)\n","    num_flip = len(flips)\n","    num_shft = len(shifts)\n","    \n","    # Generate the augmented training dataset\n","    num_training = images_training.shape[0]\n","    num_augmented = num_training * num_rota * num_flip * num_shft\n","    images_augmented = np.empty(shape=(num_augmented, 3, training_resize, training_resize))\n","    labels_augmented = np.empty(shape=(num_augmented, 1, training_resize, training_resize))\n","    print(f\"images_augmented.shape = {images_augmented.shape}\")\n","    print(f\"labels_augmented.shape = {labels_augmented.shape}\")\n","    \n","    counter = 0\n","    for index in tqdm(range(num_training)):\n","        image = np.transpose(images_training[index], (1, 2, 0))\n","        label = np.transpose(labels_training[index], (1, 2, 0))\n","        for rota in rotations:\n","            for flip in flips:\n","                for shft in shifts:\n","                    # Rotate a resized training pair\n","                    image_rota = scipy.ndimage.rotate(image, rota, reshape=False, mode='reflect')\n","                    label_rota = scipy.ndimage.rotate(label, rota, reshape=False, mode='reflect')\n","                    \n","                    # Flip the rotated training pair\n","                    if flip == 'original':\n","                        image_flip = image_rota\n","                        label_flip = label_rota\n","                    else:\n","                        image_flip = flip(image_rota)\n","                        label_flip = flip(label_rota)\n","                    \n","                    # Shift the flipped training pair\n","                    shft_H = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n","                    shft_W = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n","                    image_shft = scipy.ndimage.shift(image_flip, (shft_H, shft_W, 0), mode='reflect')\n","                    label_shft = scipy.ndimage.shift(label_flip, (shft_H, shft_W, 0), mode='reflect')\n","                    \n","                    images_augmented[counter] = np.clip(np.transpose(image_shft, (2, 0, 1)), 0, 1)\n","                    labels_augmented[counter] = np.transpose(label_shft, (2, 0, 1)) > 0.3\n","                    counter += 1\n","    \n","    # Permute the augmented training dataset randomly\n","    permuted_sequence = np.random.permutation(num_augmented)\n","    images_augmented = images_augmented[permuted_sequence]\n","    labels_augmented = labels_augmented[permuted_sequence]\n","    \n","    return images_augmented, labels_augmented"],"id":"7ee7b917"},{"cell_type":"code","execution_count":null,"metadata":{"id":"91f46696"},"outputs":[],"source":["if training_data_processing:\n","    # Load and generate the resized training dataset and validation dataset\n","    images_training, labels_training, images_validation, labels_validation = training_data_loading(path_training,\n","                                                                                                   training_resize,\n","                                                                                                   training_number)\n","    # Generate the augmented training dataset\n","    rotations = [0, 45, 90, 135] # the rotation angle\n","    \n","    flips = ['original', np.flipud, np.fliplr] # 'original', np.flipud, np.fliplr\n","\n","    shifts = [(-16, 16)]\n","    \n","    images_augmented, labels_augmented = training_data_augmentation(images_training, \n","                                                                    labels_training, \n","                                                                    rotations, \n","                                                                    flips, \n","                                                                    shifts, \n","                                                                    training_resize)\n","    # Save the augmented training dataset and resized validation dataset\n","    # to your Google Drive or local file system\n","    np.save(f'{path_data}images_training', images_augmented)\n","    np.save(f'{path_data}labels_training', labels_augmented)\n","    np.save(f'{path_data}images_validation', images_validation)\n","    np.save(f'{path_data}labels_validation', labels_validation)\n","elif not model_loading:\n","    # Load the augmented training dataset and resized validation dataset\n","    # from your Google Drive or local file system\n","    images_augmented = np.load(f'{path_data}images_training.npy')\n","    labels_augmented = np.load(f'{path_data}labels_training.npy')\n","    images_validation = np.load(f'{path_data}images_validation.npy')\n","    labels_validation = np.load(f'{path_data}labels_validation.npy')"],"id":"91f46696"},{"cell_type":"markdown","metadata":{"id":"0ac8f763"},"source":["# Train the Instance of the Neural Network"],"id":"0ac8f763"},{"cell_type":"code","execution_count":null,"metadata":{"id":"67068793"},"outputs":[],"source":["class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, outputs, targets, smooth=0):\n","        \"\"\"\n","        DiceBCELoss - Compute the Dice-BCE Loss.\n","        Args:\n","            outputs (tensor): output tensor\n","            targets (tensor): target tensor\n","        Returns:\n","            dice_BCE_loss (tensor): the Dice-BCE Loss\n","        \"\"\"\n","        # Flatten output and target tensors\n","        outputs = outputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        # Compute the dice Loss\n","        intersection = (outputs * targets).sum()                           \n","        dice_loss = 1 - (2. * intersection + smooth) / (outputs.sum() + targets.sum() + smooth)\n","        \n","        # Compute the standard binary cross-entropy (BCE) loss\n","        BCE_loss = self.bce_loss(outputs, targets)\n","        \n","        dice_BCE_loss = dice_loss + BCE_loss\n","        \n","        return dice_BCE_loss\n","\n","\n","class BCEIoULoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(BCEIoULoss, self).__init__()\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, outputs, targets, beta=0.6, alpha=0.25, gamma=3, smooth=0):\n","        \"\"\"\n","        BCEIoULoss - Compute the BCEIoULoss Loss.\n","        Args:\n","            outputs (tensor): output tensor\n","            targets (tensor): target tensor\n","        Returns:\n","            BCE_IoU_loss (tensor): the BCE-IoU Loss\n","        \"\"\"\n","        # Flatten output and target tensors\n","        outputs = outputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        # Compute the intersection-over-union (IoU) loss\n","        intersection = (outputs * targets).sum()\n","        total = (outputs + targets).sum()\n","        union = total - intersection \n","        IoU_loss = 1 - (intersection + smooth) / (union + smooth)\n","        \n","        # Compute the modified BCE loss\n","        BCE_loss = self.bce_loss(outputs, targets)\n","        BCE_exp = torch.exp(-BCE_loss)\n","        modified_BCE_loss = alpha * (1 - BCE_exp) ** gamma * BCE_loss\n","        \n","        BCE_IoU_loss = beta * modified_BCE_loss + (1 - beta) * IoU_loss\n","\n","        return BCE_IoU_loss"],"id":"67068793"},{"cell_type":"code","execution_count":null,"metadata":{"id":"882daf1f"},"outputs":[],"source":["def train(model,\n","          images_training,\n","          labels_training,\n","          images_validation,\n","          labels_validation,\n","          loss_func,\n","          batch_size=8,\n","          learning_rate=1e-3,\n","          epochs=80,\n","          model_validation=False,\n","          cuda_available=True,\n","          path_model = 'models/'):\n","    \"\"\"\n","    train - Train the instance of the neural network.\n","    Args:\n","        model (torch): the instance of the neural network\n","        images_training, labels_training (numpy): the augmented training dataset\n","        images_validation, labels_validation (numpy): the resized validation dataset\n","        loss_func (class): the loss function\n","        batch_size (int): the number of samples per batch to load (default: 8)\n","        learning_rate (float): the learning rate (default: 1e-3)\n","        epochs (int): the learning epochs (default: 80)\n","        if_validation (bool): the flag indicating whether or not to implement validation (default: False)\n","        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n","    \"\"\"\n","    # Use torch.utils.data to create a training_generator\n","    images_training = torch.Tensor(images_training)\n","    labels_training = torch.Tensor(labels_training)\n","    training_set = TensorDataset(images_training, labels_training)\n","    training_generator = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n","\n","    # Use torch.utils.data to create a validation_generator\n","    if model_validation and len(images_validation) > 0:\n","        images_validation = torch.Tensor(images_validation)\n","        labels_validation = torch.Tensor(labels_validation)\n","        validation_set = TensorDataset(images_validation, labels_validation)\n","        validation_generator = DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n","\n","    # Implement Adam algorithm\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    # Decay the learning rate by gamma every step_size epochs.\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5, verbose=True)\n","\n","    # Loop over epochs\n","    for epoch in tqdm(range(epochs)):\n","        # Training\n","        print(f'\\n---------Training for Epoch {epoch + 1} starting:---------')\n","        model.train()\n","        loss_training = 0\n","        # Loop over batches in an epoch using training_generator\n","        for index, (inputs, labels) in enumerate(training_generator):\n","            if cuda_available:\n","                # Transfer to GPU\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","            \n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_func(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            loss_training += loss\n","\n","            if index % 20 == 0:\n","                loss_item = loss.item()\n","                print(f'→ Running_loss for Batch {index + 1}: {loss_item}')\n","        \n","        print(f'\\033[1mTraining loss for Epoch {epoch + 1}: {loss_training}\\033[0m\\n')\n","\n","        if model_validation and len(images_validation) > 0:\n","            # Validation\n","            print(f'--------Validation for Epoch {epoch + 1} starting:--------')\n","            model.eval()\n","            with torch.no_grad():\n","                loss_validation = 0\n","                # Loop over batches in an epoch using validation_generator\n","                for index, (inputs, labels) in enumerate(validation_generator):\n","                    if cuda_available:\n","                        # Transfer to GPU\n","                        inputs, labels = inputs.cuda(), labels.cuda()\n","                \n","                    outputs = model(inputs)\n","                    loss_validation += loss_func(outputs, labels)\n","                \n","            print(f'\\033[1mValidation loss for Epoch {epoch + 1}: {loss_validation}\\033[0m\\n')\n","\n","        scheduler.step()\n","                    \n","        torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': loss_func,\n","                }, path_model + 'model.model')"],"id":"882daf1f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"33caad3b"},"outputs":[],"source":["if model_training:\n","    print(f\"\\nimages_training.shape = {images_augmented.shape}\")\n","    print(f\"labels_training.shape = {labels_augmented.shape}\")\n","    print(f\"images_validation.shape = {images_validation.shape}\")\n","    print(f\"labels_validation.shape = {labels_validation.shape}\")\n","\n","    train(model,\n","          images_augmented,\n","          labels_augmented,\n","          images_validation,\n","          labels_validation,\n","          loss_func=BCEIoULoss(), # BCEIoULoss(), DiceBCELoss(), nn.BCELoss()\n","          batch_size=8,\n","          learning_rate=1e-3,\n","          epochs=80,\n","          model_validation=model_validation,\n","          cuda_available=cuda_available,\n","          path_model=path_model)"],"id":"33caad3b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4r7atglYKv4-"},"outputs":[],"source":["if model_loading:\n","    # Load the model from your Google Drive or local file system\n","    checkpoint = torch.load(path_model + 'model.model')\n","    model.load_state_dict(checkpoint['model_state_dict'])"],"id":"4r7atglYKv4-"},{"cell_type":"markdown","metadata":{"id":"A_Evk4YfAyK7"},"source":["# Process the Testing Dataset and Create the Submission File"],"id":"A_Evk4YfAyK7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6e6fd77"},"outputs":[],"source":["def testing_patch_extracting(input, trar=384, tesr=584):\n","    \"\"\"\n","    testing_patch_extracting - Divide each resized testing image into four patches, one at each corner.\n","    Args:\n","        input (numpy) - the resized testing image\n","        trar (int) - the resolution of resized training images and the corresponding masks\n","        tesr (int) - the resolution of resized testing images\n","    Returns:\n","        input_patches (numpy) - the four patches\n","    \"\"\"\n","    if tesr / 2 > trar:\n","        raise AssertionError(\"training_resize is too small.\")\n","\n","    input_patches = np.empty(shape=(4, input.shape[2], trar, trar))\n","    input_patches[0] = np.transpose(input[0:0+trar, 0:0+trar, :], (2, 0, 1))\n","    input_patches[1] = np.transpose(input[0:0+trar, tesr-trar:tesr, :], (2, 0, 1))\n","    input_patches[2] = np.transpose(input[tesr-trar:tesr, 0:0+trar, :], (2, 0, 1))\n","    input_patches[3] = np.transpose(input[tesr-trar:tesr, tesr-trar:tesr, :], (2, 0, 1))\n","    \n","    return input_patches\n","\n","\n","def testing_patch_assembling(output_patches, trar=384, tesr=584):\n","    \"\"\"\n","    testing_patch_assembling - Merge the four masks into one resized mask.\n","    Args:\n","        output_patches (numpy) - the masks of the four patches\n","        trar (int) - the resolution of resized training images and the corresponding masks\n","        tesr (int) - the resolution of resized testing images\n","    Returns:\n","        output (numpy) - the resized mask\n","    \"\"\"\n","    # The extracting length\n","    eL = int(tesr / 2)\n","\n","    output = np.empty(shape=(output_patches.shape[1], tesr, tesr))\n","    output[:, 0:eL, 0:eL] = output_patches[0, :, 0:eL, 0:eL]\n","    output[:, 0:eL, tesr-eL:tesr] = output_patches[1, :, 0:eL, trar-eL:trar]\n","    output[:, tesr-eL:tesr, 0:eL] = output_patches[2, :, trar-eL:trar, 0:eL]\n","    output[:, tesr-eL:tesr, tesr-eL:tesr] = output_patches[3, :, trar-eL:trar, trar-eL:trar]\n","    \n","    return output\n","\n","def mask_to_submission(output, index):\n","    \"\"\"\n","    mask_to_submission - Convert the mask of each testing image into the submission format.\n","    Args:\n","        output (numpy) - the mask of the testing image\n","        index (int) - the index of the testing image\n","    Returns:\n","        mask_submission (list) - the submission format of the mask\n","    \"\"\"\n","    mask_submission = []\n","    for i in range(0, output.shape[0], 16):\n","        for j in range(0, output.shape[1], 16):\n","            prediction = 0\n","            patch = output[j:j+16, i:i+16]\n","            if np.mean(patch > 0.2) > 0.25:\n","                prediction = 1\n","            mask_submission.append([\"{:03d}_{}_{}\".format(index, i, j), prediction])\n","    return mask_submission\n","\n","\n","def submission_creating(model, path_testing='test_set_images/', training_resize=384, testing_resize=584, cuda_available=True):\n","    \"\"\"\n","    submission_creating - Load and generate the resized training dataset and validation dataset.\n","    Args:\n","        model (torch): the instance of the neural network\n","        path_testing (str): the location in your Google Drive or local file system\n","        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs) (default: 384)\n","        testing_resize (int): the resolution of resized testing images (default: 584)\n","        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n","    Returns:\n","        submission (numpy): the final submission file\n","    \"\"\"\n","    submit_outputs = []\n","    for index in tqdm(range(1, 51)):\n","        model.eval()\n","        # Load a testing image\n","        input = np.array(Image.open(f'{path_testing}/test_{index}/test_{index}.png')).astype('float32') / 255\n","\n","        # Resize the testing image\n","        input = resize(input, (testing_resize, testing_resize))\n","\n","        # Divide the resized testing image into four patches, one at each corner.\n","        input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n","        input_patches = torch.from_numpy(input_patches).float()\n","\n","        # Predict the mask of the four patches\n","        if cuda_available:\n","            output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n","        else:\n","            output_patches = model(input_patches).detach().numpy()\n","        \n","        # Merge the four masks into one resized mask\n","        output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n","\n","        # Restore the resized mask to the original resolution\n","        output = resize(output, (608, 608))\n","\n","        # Convert the mask of the testing image into the submission format\n","        submit_output = mask_to_submission(output, index)\n","\n","        submit_outputs.append(submit_output)\n","\n","    submission = np.concatenate(submit_outputs, axis=0)\n","    submission = np.concatenate(([['id', 'prediction']], submission), axis=0)\n","\n","    return submission"],"id":"f6e6fd77"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcc3e105"},"outputs":[],"source":["submission = submission_creating(model, \n","                                 path_testing, \n","                                 training_resize,\n","                                 testing_resize,\n","                                 cuda_available)\n","\n","np.savetxt(\"submit.csv\", submission, delimiter=\",\", fmt = '%s')\n","if use_google_colab:\n","    files.download('submit.csv')"],"id":"fcc3e105"},{"cell_type":"code","execution_count":null,"metadata":{"id":"24c4b4cb"},"outputs":[],"source":["index = 23\n","model.eval()\n","\n","# Load a testing image\n","input = np.array(Image.open(f'{path_testing}/test_{index}/test_{index}.png')).astype('float32') / 255\n","\n","# Resize the testing image\n","input = resize(input, (testing_resize, testing_resize))\n","\n","# Divide the resized testing image into four patches, one at each corner.\n","input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n","input_patches = torch.from_numpy(input_patches).float()\n","\n","# Predict the mask of the four patches\n","if cuda_available:\n","    output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n","else:\n","    output_patches = model(input_patches).detach().numpy()\n","\n","# Merge the four masks into one resized mask\n","output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n","\n","# Restore the resized mask to the original resolution\n","output = resize(output, (608, 608))\n","\n","f, axs = plt.subplots(1, 2)\n","axs[0].imshow(input)\n","axs[1].imshow(output)"],"id":"24c4b4cb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Buz2mE2DA5pL"},"outputs":[],"source":["im = Image.fromarray(output * 255)\n","im = im.convert(\"L\")\n","im.save(\"mask.png\")\n","if use_google_colab:\n","    files.download('mask.png')"],"id":"Buz2mE2DA5pL"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"DLinkNet50.ipynb","toc_visible":true,"provenance":[{"file_id":"1Z_Quu0J5U1J1dJzr4sAcYFkFJp2Tl2wR","timestamp":1640256632670},{"file_id":"1l54CZUKLbCpPE6KGacFpYsd559W9KER7","timestamp":1640256571217},{"file_id":"1i_hTd12pCfVGEf3i-UeYeJW1trUuk_CT","timestamp":1640256050805}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}